# Analisi_Wikipedia
This repository is the tenth project of my master in Data Science

Questo progetto si colloca alla fine del modulo dedicatao ai Big Data alle loro caratteristiche e problematiche. Durante il corso oltre ad alcuni cenni storici sono stati introdotte le differenze tra calcolo parallelo e distribuito, tecnologie come Apache Spark e Apache Zeppelin. Molto spazio è stato dedicato a Databricks e alla spiegazione delle caratteristiche degli RDD (Resilient Distributed Dataset) oltre che di Spark SQL e MLlib. Alla fine del modulo è stata fatta una panoramica su cosa sono Data Lake, Data Warehouse e i diversi servizi di Data Storage, oltre ad un mini-progetto di utilizzo di Spark Streaming per l'analisi in Real Time.

Il progetto richiedeva di studiare un dataset con articoli presi da Wikipedia al fine di ottimizzare l'analisi e la categorizzazione dei contenuti di Wikipedia al fine di sviluppare una sistema di classificazione automatica. I punti svolti in questo progetto sono stati:

- Analisi esplorativa: per questa fase ho determinato il conteggio di articoli per categoria (ho trovato 15 categorie con percentuali di rappresentanza molto sbilanciate), il numero medio di parole per categoria, la lunghezza dell'articolo più lungo e del più corto per categoria e ho generato una nuvola di parole per ogni categoria, per creare le nuvole di parole ho prima svolto una pulizia del testo rimuovendo caratteri speciali e stopwords per far emergere le parole caratterizzanti della categoria.
- Sviluppo di un classificatore automatico: ho definito due classificatori: una regressione logistica  e una random forest. Come feature ho usato la concatenazione delle colonne summary e documents e come metriche ho osservato l'accuracy e l'F1-score definite per un classificatore multiclasse. Ho definito delle pipeline: RegexTokenizer, StopWordsRemover, StringIndexer (per mappare le categorie), HashingTF, IDF e poi il modello una volta la regressione logistica e l'altra la random forest. Il fine tuning dei modelli è stato eseguito solo sul 10% del dataset per motivi computazionali (è stata usata la versione comunity di Databricks), ho riscontrato che il modello con le performance più alta è stata la regressione logistica. Per valutare al meglio i risultati del mio modello ho generato anche una matrice di confusione che evidenziata come oltre alla categoria politics (scarsamente rappresentata nel dataset) per il resto il modello si è ben comportato. Ho osservato che in molti caso il mislabel era tra categorie affini come per esempio medicina e ricerca.

Framework utilizzato: PySpark su Databricks
